{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b5cdb87-2378-49e9-b626-2f47565a5416",
   "metadata": {},
   "source": [
    "## RAG2\n",
    "\n",
    "This version is similar to RAG1\n",
    "\n",
    "+ In RAG1, the first step is LLM with a structured output( llm.with_structued_output)\n",
    "  + In RGA2, the first step is LLM with tools bound (llm.bind_tools). By this way, the execution can be routed to tool node\n",
    "+ in RAG1, tool call (searching vector store) is a explicit step, just like other step.\n",
    "  +  In RAG2, the tool is wrapped through tool node. The excution is routed to it through a tools_conditions. The tools args are\n",
    "     structured output.\n",
    "+ in RAG1, we combine all the output and generate a response in natural language\n",
    "  + in RAG2, we again combine all the outpus and generate a respose in natual language.\n",
    " \n",
    "+ RAG1 has a well-defined State while RAG2 uses MessagesState. In real application, we may want to use well-defined state, so we can compose a unique prompt at each step based on the data from state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab1b5193-d631-44fd-a3ac-113000af25cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Basic set up : create chat mode, vector store\n",
    "\n",
    "from langchain_google_vertexai import ChatVertexAI, VertexAIEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "llm = ChatVertexAI(model=\"gemini-2.0-flash-001\")\n",
    "embeddings = VertexAIEmbeddings(model=\"text-embedding-004\")\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaf17ad9-21c2-4d88-8953-39ba85be8864",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "## Index documents. This is the basis of all our queries. \n",
    "## This is like database setup. \n",
    "import bs4\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = WebBaseLoader(web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "                      bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "                          class_=(\"post-content\", \"post-header\", \"post-title\")\n",
    "                      )))\n",
    "\n",
    "docs = loader.load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, add_start_index=True)\n",
    "splitts = splitter.split_documents(documents=docs)\n",
    "_=vector_store.add_documents(splitts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17b5a14-0f4e-4473-89a2-e94c0968e613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to build a graph here, \n",
    "\n",
    "# Graph always needs State, MessagesState is so pupular, so it is provided\n",
    "from langgraph.graph import START, END, MessagesState, StateGraph\n",
    "# Tool call is part \n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from typing import Annotated\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "#The tool is just a regular python function with input spec.\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve( query: Annotated[str, ..., \"The query to search external source\"]):\n",
    "    \"\"\"Retrieve documents based on the query string.\"\"\"\n",
    "    \n",
    "    docs = vector_store.similarity_search(query, k=2)\n",
    "\n",
    "    # summary text\n",
    "    text = \"\\n\\n\".join(f\"source: {doc.metadata['source']} \\n\\n content: {doc.page_content} \" for doc in docs)\n",
    "\n",
    "    return text, docs\n",
    "\n",
    "\n",
    "#step 1 Query -> [Tool call | END]\n",
    "def check_query(msg_state:MessagesState):\n",
    "    # only after bind tools. The llm can output a ToolCall \n",
    "    # The toolCall will be invoked by graph\n",
    "    llm_with_tools = llm.bind_tools([retrieve])\n",
    "    # give all the message to llm\n",
    "    ai_msg = llm_with_tools.invoke(msg_state['messages'])\n",
    "    return {'messages': [ai_msg]}\n",
    "\n",
    "# step 2. tool call\n",
    "tools = ToolNode([retrieve])\n",
    "\n",
    "#step 3. Generate text based on tool call results\n",
    "\n",
    "def gen(msg_state:MessagesState):\n",
    "    #retreive the messages from tool call.\n",
    "\n",
    "    last_tool_msg = None\n",
    "    for msg in reversed(msg_state['messages']):\n",
    "        if msg.type == 'tool':\n",
    "            last_tool_msg = msg\n",
    "            break\n",
    "\n",
    "    # \n",
    "    docs_content = last_tool_msg.content\n",
    "    system_message_content = (\n",
    "        \"You are an assistant for question-answering tasks. \"\n",
    "        \"Use the following pieces of retrieved context to answer \"\n",
    "        \"the question. If you don't know the answer, say that you \"\n",
    "        \"don't know. Use three sentences maximum and keep the \"\n",
    "        \"answer concise.\"\n",
    "        \"\\n\\n\"\n",
    "        f\"{docs_content}\"\n",
    "    )\n",
    "\n",
    "    msgs = [msg \n",
    "            for msg in msg_state['messages']\n",
    "            if msg.type in ('human', 'system') # keep human and system\n",
    "            or (msg.type =='ai' and not msg.tool_calls) # keep ai, but not those for tool calls. tool calls already ececuted\n",
    "           ]\n",
    "    ans=llm.invoke([SystemMessage(system_message_content)]+ msgs)\n",
    "    return {'messages': [ans]}\n",
    "\n",
    "graph_builder = StateGraph(MessagesState)\n",
    "graph_builder.add_node(check_query)\n",
    "graph_builder.add_node(tools)\n",
    "graph_builder.add_node(gen)\n",
    "graph_builder.set_entry_point(\"check_query\")\n",
    "graph_builder.add_conditional_edges(\"check_query\", tools_condition, {END:END, \"tools\": \"tools\"})\n",
    "graph_builder.add_edge(\"tools\", \"gen\")\n",
    "graph_builder.add_edge(\"gen\", END)\n",
    "graph=graph_builder.compile()\n",
    "\n",
    "from IPython.display import display, Image\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c98dc2-5a48-4f0d-ac0c-73b27a1009e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_TRACING_V2\"] = \"true\"\n",
    "\n",
    "for step in graph.stream( {\"messages\": [{\"role\": \"user\", \"content\": \"What is Task Decomposition?\"}]}, stream_mode=\"values\"):\n",
    "    print(step['messages'][-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09db4133-1311-481d-ba3a-f943baf3eec5",
   "metadata": {},
   "source": [
    "## How do memory\n",
    "\n",
    "To remember what graph already did, \n",
    "\n",
    "1.  We give a checkpointer when building graph through compiling. This is like destionation (storage) of the project to save the state.\n",
    "2.  We give a configured thread_id when invoke graph. This is like a key in the storage (such as user id)\n",
    "\n",
    "\n",
    "## What does memory do.\n",
    "Once there is a memory to save the state, the state is loaded from the storage. Then we could build a unique context using the stored state for the current graph execution. \n",
    "\n",
    "In this example,  the new human question is appended to all existing messages. We then select all human and AI messages, and use this messages as prefix-prompt as context for current question. \n",
    "\n",
    "Please note here, we doesn't use multi-turn chat session from the LLM directly. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fbefe0-6f05-4d78-8498-445e7d554c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory=MemorySaver()\n",
    "graph1 = graph_builder.compile(checkpointer=memory)\n",
    "config= {\"configurable\": {\"thread_id\": \"jjrag2_1\"}}\n",
    "graph1.invoke({'messages': [{'role': 'user', 'content': 'What is Task Decomposition?'}]}, config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94aa2c9f-bdd1-4e41-8b5b-86e07f348339",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_state = graph1.invoke({'messages': [{'role': 'user', 'content': 'Can you lookup some common ways of doing it?'}]}, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da35bf7d-41bc-4ea2-88f4-b7912ecf5662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition can be done (1) by LLM with simple prompting, (2) by using task-specific instructions, or (3) with human inputs.\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg_state['messages'][-1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f294be5d-ef4d-4297-8c63-0a47047a6846",
   "metadata": {},
   "source": [
    "## Agent has the same logic has manual StateGraph\n",
    "\n",
    "+ The abstract agent decides which tool to call, and how process its result. \n",
    "+ The create_react_agent creates a compiled graph.  The graph topology is decided by agent itself. We assume the llm is smart\n",
    "   enough to split the questions into small tasks.\n",
    "\n",
    "In our example below, the agent apparenly can not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200f0860-7b66-451b-9835-4e95027aeb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "memory1 = MemorySaver()\n",
    "config1= {\"configurable\": {\"thread_id\": \"jjrag2_2\"}}\n",
    "agent_executor = create_react_agent(llm, [retrieve], checkpointer=memory1)\n",
    "question=\"What is Task Decomposition?\\n\\n Once you get the answer, look up some common ways of doing it.\"\n",
    "for step_state in agent_executor.stream({'messages': [{'role': 'user', 'content': question}]}, stream_mode=\"values\", config=config1):\n",
    "    step_state['messages'][-1].pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4760db90-ffb9-4cd8-ba80-53e6b821fc98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
