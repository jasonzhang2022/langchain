{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08db1c75-0ff7-4096-ba8a-6abd7e69d146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LANGSMITH_TRACING']='true'\n",
    "os.environ['LANGSMITH_TRACING_V2']='true'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143c6bce-b0d9-4a9a-8706-6c2deef4ae8f",
   "metadata": {},
   "source": [
    "## Summarization\n",
    "\n",
    "This is a relatively easy process in LLM, however, the map and reduce approach show how we could build a complex execution graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78e63ba-6637-4381-bc0a-35e07f960e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach one, we just concatethe document into the  context and ask the llm to summarize it\n",
    "# This call stuff\n",
    "\n",
    "#load basic \n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "docs = loader.load()\n",
    "\n",
    "content = \"\"\n",
    "for doc in docs: \n",
    "    content += \"\\n\\n\"+doc.page_content\n",
    "\n",
    "\n",
    "llm = ChatVertexAI(model=\"gemini-2.0-flash-001\")\n",
    "\n",
    "# We have to have user message. This does not work if we change the user role to system\n",
    "prompt = ChatPromptTemplate([(\"user\", \"Please summarize the following text:\\n\\n {context}\" )])\n",
    "chain = prompt |llm\n",
    "\n",
    "summary = chain.invoke({'context': content})\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "display(Markdown(summary.content))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc44f6d-90b4-41cf-9a50-3cb721625906",
   "metadata": {},
   "source": [
    "## Map Reduce Approach\n",
    "\n",
    "\n",
    "1. We need to split the documents into small chunk, each chunk should be less than the LLM max token.\n",
    "2. The STARt to the first node has   a condition which emit a generate summary for each chunk.\n",
    "3. We the collect summarization or collapse summarization. After collapse, If some of the summarization has long summarization, we continue to collapse.\n",
    "4. Finally, we generate the final summary from collected summrizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47e122a5-1e8d-448a-b2fe-c8ce23f9b8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1003, which is longer than the specified 1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare and split the documents into chunk\n",
    "\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=1000, chunk_overlap=0)\n",
    "splitts = splitter.split_documents(docs)\n",
    "len(splitts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58e384a-d017-4213-b464-2fa65f4c256e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c23c6c4e-895e-4f94-9f3b-30d85336cd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the graph\n",
    "import operator\n",
    "from typing import Annotated, List, TypedDict, Literal\n",
    "from langgraph.constants import Send\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "from langchain.chains.combine_documents.reduce import (\n",
    "    acollapse_docs,\n",
    "    split_list_of_docs,\n",
    ")\n",
    "\n",
    "token_max=1000\n",
    "\n",
    "def length_function(documents: list[Document]):\n",
    "    \"\"\"How many tokens for all the the input docs.\"\"\"\n",
    "    return sum(llm.get_num_tokens(doc.page_content) for doc in documents)\n",
    "\n",
    "\n",
    "class OverallState(TypedDict):\n",
    "    splitts: List[str] # original inputs\n",
    "    summaries: Annotated[list, operator.add] # a summaries for each splits\n",
    "    collapsed_summaries: List[Document] # convert summaries to document, and collapse if needed.\n",
    "    final_summary: str # final output\n",
    "\n",
    "\n",
    "\n",
    "class SummaryContent(TypedDict):\n",
    "    text: str\n",
    "\n",
    "# first step, generate a summary for each document\n",
    "async def generate_summary(input: SummaryContent):\n",
    "    ai_msg= await llm.ainvoke([\n",
    "        ('system', \"please write a concise summary for the text from user. Only output the summary\"),\n",
    "        ('human', input['text'])\n",
    "    ])\n",
    "    # The return should be a field in the OverallState since the generate_summary\n",
    "    # is one Node in the overall graph.\n",
    "    return {'summaries': [ai_msg.content]}\n",
    "\n",
    "# Map each splits to a summary node\n",
    "def map_summaries(state: OverallState):\n",
    "    return [Send(\"generate_summary\", {'text': split}) for split in state['splitts'] ]\n",
    "\n",
    "# Convert the summary text to documents. \n",
    "def collect_summaries(state:OverallState):\n",
    "    return {\"collapsed_summaries\": [Document(summary) for summary in state['summaries']]}\n",
    "\n",
    "\n",
    "reduce_template = \"\"\"\n",
    "The following is a set of summaries:\n",
    "{docs}\n",
    "Take these and distill it into a final, consolidated summary\n",
    "of the main themes.\n",
    "\"\"\"\n",
    "\n",
    "reduce_prompt = ChatPromptTemplate([(\"human\", reduce_template)])\n",
    "\n",
    "# reduce a set of document into one\n",
    "async def _reduce(input: dict) -> str:\n",
    "    prompt = reduce_prompt.invoke(input)\n",
    "    response = await llm.ainvoke(prompt)\n",
    "    return response.content\n",
    "\n",
    "async def collapse_summaries(state:OverallState):\n",
    "    doc_lists = split_list_of_docs(\n",
    "        state['collapsed_summaries'], length_function, token_max\n",
    "    )\n",
    "    results =[]\n",
    "    for doc_list in doc_lists:\n",
    "        results.append(await acollapse_docs(doc_list, _reduce))\n",
    "\n",
    "    return {\"collapsed_summaries\": results}\n",
    "\n",
    "def should_collapse(state:OverallState)->Literal[\"collapse_summaries\", \"generate_final_summary\"]:\n",
    "    num_tokens = length_function(state['collapsed_summaries'])\n",
    "    if num_tokens > token_max:\n",
    "        return \"collapse_summaries\"\n",
    "    else:\n",
    "        return \"generate_final_summary\"\n",
    "\n",
    "async def generate_final_summary(state:OverallState):\n",
    "    summary = await _reduce(state['collapsed_summaries'])\n",
    "    return {'final_summary': summary}\n",
    "\n",
    "builder = StateGraph(OverallState)\n",
    "builder.add_node('generate_summary', generate_summary)\n",
    "builder.add_node('collect_summaries', collect_summaries)\n",
    "builder.add_node('collapse_summaries', collapse_summaries)\n",
    "builder.add_node('generate_final_summary', generate_final_summary)\n",
    "\n",
    "builder.add_conditional_edges(START, map_summaries, ['generate_summary'])\n",
    "builder.add_edge('generate_summary', 'collect_summaries')\n",
    "builder.add_conditional_edges('collect_summaries', should_collapse)\n",
    "builder.add_conditional_edges('collapse_summaries', should_collapse)\n",
    "builder.add_edge('generate_final_summary', END)\n",
    "graph = builder.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "803d7de5-5dff-494b-a943-83187be4ed8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['generate_summary'])\n",
      "dict_keys(['generate_summary'])\n",
      "dict_keys(['generate_summary'])\n",
      "dict_keys(['generate_summary'])\n",
      "dict_keys(['generate_summary'])\n",
      "dict_keys(['generate_summary'])\n",
      "dict_keys(['generate_summary'])\n",
      "dict_keys(['generate_summary'])\n",
      "dict_keys(['generate_summary'])\n",
      "dict_keys(['generate_summary'])\n",
      "dict_keys(['generate_summary'])\n",
      "dict_keys(['generate_summary'])\n",
      "dict_keys(['generate_summary'])\n",
      "dict_keys(['generate_summary'])\n",
      "dict_keys(['collect_summaries'])\n",
      "dict_keys(['collapse_summaries'])\n",
      "dict_keys(['generate_final_summary'])\n"
     ]
    }
   ],
   "source": [
    "async for step in graph.astream( {'splitts':[split.page_content for split in splitts]}):\n",
    "    print(step.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f3446c6-25d5-486a-bce2-7a8231aac934",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await graph.ainvoke( {'splitts':[split.page_content for split in splitts]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be111830-d7d7-47c4-826b-fe0b19726006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "LLM-powered autonomous agents are an emerging field using large language models for planning, memory, and tool use to tackle complex tasks. Planning involves task decomposition and self-reflection to improve reasoning, while memory utilizes both short-term (in-context learning) and long-term (external vector stores). Tool use expands capabilities through external APIs, with architectures like MRKL and Toolformer being developed. Challenges include efficiency, reliance on long context windows, stability, and self-evaluation issues. Applications are being explored in areas like drug discovery and autonomous scientific experiment design, alongside simulations of human behavior. A related system, GPT-Engineer, generates code repositories from natural language, often using MVC architecture. However, challenges remain with LLM agents, including context length limits, long-term planning difficulties, error adaptation, and reliability issues with natural language interfaces. Continuous self-evaluation and learning from feedback are key for optimal performance.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(result['final_summary']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1d66cc-f516-4f73-97aa-9bb2a916d627",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
